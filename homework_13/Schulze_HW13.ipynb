{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scott Schulze's second code review (I think)?\n",
    "# HAS Tools Homework 13\n",
    "# 11/21/20\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "Welcome Laura, \n",
    "This is easily THE MOST stressful assignment of the semester so far, only because now do I have to do everything we have done, but do it all neatly, and in one place! AHHH! I understand that this assignment should be comforting in that we can all demonstrate all we have learned throughout the semester, but, if you'll pardon the desert pun, in my case it is tapping a dry well. One required item I am fairly certain that you will not see is a map, I never got my head around that after all the issues.\n",
    "The purpose is to demonstrate many of the topics we've covered in a uniform, easy to ready story-board like presentation. Hopefully by the end, there will have been a function used(we're still not friends, functions and I), two different kinds of plots (they may well be the same data, just viewed two different ways), my forecasts for weeeks 1, 2 and the 16 week. This all will be headed by the discussion immediately following this of where I stand currently and how the forecasting has been going to date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it is made: Forecasting episode\n",
    "When the semester started out, week 1 especially I made my forecasts just from my own guessing. I am good at recognizing patterns and adjusting. Through the middle part of the course, I used my model for generating the forecasts this was very effective when flows were so low. Now that they have returned to near normal levels, I am back to forecasting from my knowledge as I don't have the wherewithall to change my code to better suit the current conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start with some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import urllib.request as req\n",
    "import urllib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Now the function\n",
    "    This function is given the previous week's flow value as an input\n",
    "    Then uses the previously calculated model intercept and coefficient\n",
    "    to find the first week's prediction. Then uses the first prediction to\n",
    "    make the predicition for the second week's forecast. Before returning\n",
    "    both forecast values. Reverted to a 2-week forecast on 10/24/2020.\n",
    "\n",
    "    Variables:\n",
    "    previous_flow: previous_flow is given to the function as the average flow\n",
    "    from the previous week and is the initialization point for\n",
    "    prediction: prediction is an array, set to the size of the forecasts being\n",
    "    generated, the outputs from the model are stored here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(previous_flow):\n",
    " \n",
    "    prediction = np.zeros(16)\n",
    "    for i in range(len(prediction)):\n",
    "        if i == 0:\n",
    "            prediction[i] = model.intercept_ + model.coef_ * previous_flow\n",
    "        elif i == 1:\n",
    "            prediction[i] = model.intercept_ + model.coef_ * prediction[i-1]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next the data to work with, brought in via API\n",
    "The first dataset will be from a weather observing station near to the streamgauge, the second will be the streamgage data to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "start=199701010000&end=202011210000&obtimezone=UTC&vars=air_temp&stids=QVDA3&units=temp%7CF&token=8a9ebdf8eaba4686b060300750c1658a\nhttp://api.mesowest.net/v2/stations/timeseries?start=199701010000&end=202011210000&obtimezone=UTC&vars=air_temp&stids=QVDA3&units=temp%7CF&token=8a9ebdf8eaba4686b060300750c1658a\n"
     ]
    }
   ],
   "source": [
    "mytoken = '8a9ebdf8eaba4686b060300750c1658a'\n",
    "base_url = \"http://api.mesowest.net/v2/stations/timeseries\"\n",
    "args = {\n",
    "    'start': '199701010000',              \n",
    "    'end': '202011210000',                \n",
    "    'obtimezone': 'UTC',\n",
    "    'vars': 'air_temp',\n",
    "    'stids': 'QVDA3',\n",
    "    'units': 'temp|F',\n",
    "    'token': mytoken}\n",
    "apiString = urllib.parse.urlencode(args)\n",
    "print(apiString)\n",
    "fullUrl = base_url + '?' + apiString\n",
    "print(fullUrl)\n",
    "response = req.urlopen(fullUrl)\n",
    "responseDict = json.loads(response.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now put it in a more useful format for us\n",
    "The following block takes the imported data and puts it into a pandas dataframe for us to utilize much more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'air_temp' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0499b7180ed8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Temperature'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mair_temp\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdateTime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata_weekly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'W'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_weekly\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlwt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_weekly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Last week's average temperature was \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlwt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"째F.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'air_temp' is not defined"
     ]
    }
   ],
   "source": [
    "data2 = pd.DataFrame({'Temperature': air_temp}, index=pd.to_datetime(dateTime))\n",
    "data_weekly = data2.resample('W').mean().round(1)\n",
    "print(len(data_weekly))\n",
    "lwt = data_weekly.tail(1).round(1)\n",
    "print(\"Last week's average temperature was \", lwt, \"째F.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now bring in the flow data in a similar manner\n",
    "This section brings in streamgage data from the USGS and formats it, puts it into a dataframe and standardizes by using the datetime function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = '09506000'\n",
    "start = '1990-01-01'\n",
    "end = '2020-11-21'      # Update this on Sunday\n",
    "url = \"https://waterdata.usgs.gov/nwis/dv?cb_00060=on&format=rdb&site_no=\" \\\n",
    "    + site + \"&referred_module=sw&period=&begin_date=\" + start \\\n",
    "    + \"&end_date=\" + end\n",
    "data = pd.read_table(url, skiprows=30, names=['agency_cd', 'site_no',\n",
    "                                              'datetime', 'flow', 'code'],\n",
    "                     parse_dates=['datetime'], index_col='datetime')\n",
    "data['year'] = pd.DatetimeIndex(data.index).year\n",
    "data['month'] = pd.DatetimeIndex(data.index).month\n",
    "data['day'] = pd.DatetimeIndex(data.index).dayofweek\n",
    "data['dayofweek'] = pd.DatetimeIndex(data.index).dayofweek\n",
    "flow_weekly = data.resample(\"W\").mean().round(2)\n",
    "flow_weekly.info()\n",
    "lwf = flow_weekly[['flow']].tail(1)\n"
   ]
  },
  {
   "source": [
    "### Build the Autoregressive model\n",
    "The first step is to time lag the flow data. Once that is accomplished, the training dataset is established. For this case, I am using the 400 lowest flows in the dataset for training. This was more effective during lower flows, but I am hoping a deeper training set will provide better results. The third step is to create the actual linear regression model itself. Finally the whole program is executed, with outputs for the forecast values to be input."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "flow_weekly['flow_tm1'] = flow_weekly['flow'].shift(1)\n",
    "flow_weekly['flow_tm2'] = flow_weekly['flow'].shift(2)\n",
    "train_low = flow_weekly[['flow', 'flow_tm1',\n",
    "                         'flow_tm2']].sort_values(by='flow', ascending=False)\n",
    "train_low_data = train_low.tail(400)\n",
    "test_low = flow_weekly[:-13][['flow', 'flow_tm1', 'flow_tm2']]\n",
    "model = LinearRegression()\n",
    "x = train_low_data['flow_tm1'].values.reshape(-1, 1)\n",
    "y = train_low_data['flow'].values\n",
    "model.fit(x, y)\n",
    "r_sq = model.score(x, y)\n",
    "print('coefficient of determination:', np.round(r_sq, 2))\n",
    "print('intercept:', np.round(model.intercept_, 2))\n",
    "print('slope:', np.round(model.coef_, 2))\n",
    "flow_prediction = predictions(lwf.values)\n",
    "for i in range(len(flow_prediction)):\n",
    "    print(i)\n",
    "    if lwt.values <= 80:\n",
    "        print(\"Please enter \", flow_prediction[i].round(1)+12,\n",
    "              \" for the week \", i+1, \" forecast.\")\n",
    "        print(\" \")\n",
    "    else:\n",
    "        print(\"Please enter \", flow_prediction[i].round(1),\n",
    "              \" for the week \", i+1, \" forecast.\")\n",
    "        print(\" \")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graphics generation\n",
    "This is where I am struggling most and will need help during office hours. I would like to make a plot of the average temperature and a plot of the average flow and one of both superimposed on each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(data_weekly.tail(13), color='green')\n",
    "plt.subplot(flow_weekly.tail(13), color='blue')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Temperature 째F')\n",
    "plt.legend(data_weekly, flow_weekly)\n",
    "plt.title('Weekly Average Temperature (째F)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build a map\n",
    "This map will be very rough, if it exists at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('hastools': conda)",
   "metadata": {
    "interpreter": {
     "hash": "cc3abb51540df4423c285a9cd94ff049794548ef59a66a161757ef2453e2c267"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}